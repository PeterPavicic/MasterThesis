
% TODO: Humanise
\section{Data}

\subsection{Polymarket} \label{sec:polymarket_data}



% # Data: Sources, Construction, and Preparation

This section documents the data used in the study and the procedures by which it was collected, curated, and prepared for analysis. The empirical setting is the sequence of Federal Open Market Committee meetings for which a corresponding Polymarket contract existed. For each meeting, the core object is a paired intraday dataset comprising a Polymarket price panel and a matched panel derived from 30-Day Federal Funds Rate futures (ZQ). The emphasis here is on precise definition of the observational unit, provenance and structure of the raw data, inclusion and exclusion rules, harmonization across sources, and the measures taken to ensure reproducibility while acknowledging the limitations inherent in partly manual data assembly.

\subsection{design and unit of analysis}

The unit of analysis is an event window anchored on a single scheduled FOMC decision. For each meeting, the observation period is defined to be the lifespan of the corresponding Polymarket market, beginning at market inception and ending upon resolution at the announcement. Within each window, two time series are constructed and then aligned. The first is a panel of Polymarket outcome token prices at intraday frequency for the discrete set of possible rate decisions offered by that market. The second is an intraday series of prices for the ZQ futures contract(s) whose delivery month covers the meeting date. Since the goal of the project is to study whether movements in the prediction market lead or lag movements in traditional interest-rate futures, all decisions about alignment, filtering, and sampling fidelity are made at the level of the event window. That is, each event window constitutes a self-contained, paired dataset representing the information environment leading up to that meeting.

Two features of this design deserve emphasis. First, the analysis is blockwise. Estimation and inference are conducted separately for each meeting, rather than pooling raw observations across disparate meetings. This design choice reflects that each meeting has a distinct information flow, calendar, and liquidity profile. Second, the study aligns the sources on days and minutes in which both can be reasonably expected to reflect contemporaneous beliefs. Since Polymarket trades continuously while futures observe exchange trading hours and weekend closures, the calendar is pruned so that non-overlapping periods, in particular weekends, are removed before any time alignment is imposed. The resulting datasets are therefore comparable both in the horizon over which expectations form and in the trading days on which those expectations can be revealed in prices.

\subsection{sources}

Two sources are used to capture market expectations. On the prediction-market side, the data are extracted from Polymarket, a categorical market platform where each outcome token trades at a price bounded between zero and one USDC and where the price of a yes-token can be read as an implied probability for the associated outcome. For each FOMC meeting, the relevant Polymarket market was identified by its human-readable slug. This identification was carried out manually because the study required precise mapping between the scheduled FOMC meeting and the correct categorical market, and because the naming of markets and their slugs can vary across time. Once identified, the historical price data for all outcome tokens within that market were retrieved at minute-level resolution or, where the raw feed is tick-by-tick, aggregated to the target frequency by selecting the last observation within each minute. The native fields retained for each observation include the timestamp, the outcome identifier, and the last traded or quoted price. Where the platform exposes both last trade and best bid/ask, the price closest to the prevailing midquote is preferred for stability; in the absence of a midquote, the last traded price is used. All monetary magnitudes are denominated in USDC, which is treated as a one-to-one proxy for USD within the scope of this study.

On the futures side, the data consist of time-stamped price observations for the 30-Day Federal Funds Rate futures contract. These were collected manually from two retail-facing platforms, TradingView and Barchart. The redundancy serves two purposes. First, gaps present in one source can often be filled by the other, which is useful around periods of low activity. Second, cross-checking prices identifies obvious anomalies, such as stale prints or misaligned timestamps. Barchart time stamps reflect Chicago local time, while TradingView and the Polymarket extractions are expressed in New York time or UTC. To avoid any systemic skew in relative timing, all futures timestamps were converted to New York time after taking account of the operative daylight saving time conventions. For each meeting window, the futures contract selected is the monthly ZQ contract whose delivery month contains the meeting date, which ensures that the futures price reflects the market’s expectation of the monthly average effective federal funds rate over a period split by the meeting day. In addition to the price, the raw fields include the timestamp and, where available, volume. Volume is retained only as a descriptive field; it plays no role in the construction of the probability series but can be informative about liquidity and the reliability of very short-horizon price changes.

The two raw sources thus differ in several structural respects: Polymarket is categorical with multiple simultaneous outcome series per meeting, while the futures price is a single scalar per timestamp. Polymarket trades continuously and globally, while ZQ trading is concentrated during exchange hours with extended-hours sessions and closures. Finally, the Polymarket asset space changes across meetings depending on which outcome buckets the market designer chose to list, while the futures instrument is standardized across meetings. The preparatory steps described below are therefore aimed at bringing these heterogeneous feeds into a common, coherent format for event-level analysis.

\subsection{and exclusion criteria and time window}

The time window for each meeting begins at the creation of the corresponding Polymarket market and ends at the FOMC announcement when the market resolves. That definition captures the period during which traders form and update beliefs specifically about that meeting’s outcome and avoids contaminating the dataset with post-resolution price dynamics or long post-event drifts. Because the Polymarket platform operates continuously and the futures market does not, all Saturdays and Sundays are excluded from the calendar prior to any merging or resampling. This weekend filter eliminates observations that would otherwise pair a change in a 24/7 market with a mechanically flat or stale price in a market that is closed, which could confound causality tests applied to short-horizon differences.

A second inclusion filter applies to the very early portion of some Polymarket markets. Immediately after launch, illiquidity can be severe, and the sum of yes-token prices across all outcomes may be far below one. To avoid including observations that do not yet reflect an equilibrated set of beliefs, the series is trimmed from the left until the cross-outcome sum first exceeds twenty cents. This threshold is deliberately conservative. It does not require the market to be fully arbitraged and perfectly summing to one, but it does require that enough dollar mass has entered the order book to produce interpretable relative prices across the outcome set. All subsequent observations are retained, subject to the other filters described here.

Finally, one meeting is excluded in its entirety due to incomplete Polymarket data in the relevant window. Because the econometric analysis is conducted separately for each event, it is not possible to impute or pool in a way that would recover a reliable intraday panel for that meeting without introducing modeling assumptions at the data stage. Excluding the affected meeting preserves the integrity of the remaining blocks and avoids bias arising from an unrepresentative reconstruction.

\subsection{cleaning, and merging}

The data from the two sources require substantial harmonization before they can be used jointly. The first step is standardization of identifiers and labels. Polymarket outcome names are provided in natural language and can vary across markets even when they refer to the same economic concept. For example, one market might label an outcome as increase by 25 basis points while another uses 25 bps hike or similar variants. To make cross-market comparisons coherent and to facilitate automated processing, outcome labels are recoded into a compact, uniform vocabulary, such as up25 for a 25-basis-point increase, down25 for a 25-basis-point decrease, and noChange for the status quo. This recoding is lossless with respect to the economics of the outcome space and is enforced uniformly across all meetings.

The second step is temporal harmonization. All timestamps are expressed in New York local time with explicit handling of daylight saving time transitions. The conversion from Chicago time for the Barchart feed is applied using Olson time-zone databases to ensure that historical DST rules are respected, rather than assuming a fixed offset. The Polymarket and TradingView feeds are already aligned to New York time or to UTC with offsets that can be converted deterministically. Once expressed in a common time zone, the series are mapped onto a shared intraday grid. The target grid for this study is one minute. If a source provides multiple ticks within a minute, the last observation within that minute is retained as the minute’s value. If a source is missing a minute because no trade occurred, a last-observation-carried-forward rule is applied within the same trading day. Carry-forward is never applied across a day boundary or across a weekend to avoid creating artificial persistence through a closure. The weekend filter ensures that both sources are blank over Saturdays and Sundays, so no fill is possible or necessary in those periods.

Before merging the sources, the Polymarket panel is reshaped from long to wide format within each meeting so that each minute is associated with a vector of prices, one for each active outcome label in that market. The cross-section of outcomes may differ across meetings, but within a given meeting the set is fixed once the early illiquidity trimming is applied. Two additional standardizations are implemented at this stage. First, prices are stored as decimals on the unit interval, and any quoted values in cents are converted accordingly. Second, for timestamps where the sum of outcome prices deviates materially from one, the vector is optionally renormalized to sum to unity. This renormalization is not used to alter the data’s substantive content but to provide a consistent basis for constructing aggregate statistics and for aligning with the probability vectors inferred from the futures side in the subsequent methodology. The raw, unnormalized prices are retained in parallel so that robustness checks can be conducted without conflating measurement with correction.


The futures series, once converted to New York time and reduced to minute bars, is merged to the Polymarket panel by timestamp. Because the futures asset is scalar while the Polymarket asset is a vector, the merged dataset contains both a time-stamped vector of outcome prices and a scalar futures price for the same minute. Where both TradingView and Barchart provide futures prices for a given minute, a reconciliation rule is applied. If the two sources agree within a tight tolerance, the TradingView print is kept as the canonical value and the Barchart value is logged for verification. If they differ materially and one source is stale while the other updates, the updating value is preferred. If they differ materially and both are timely, the observation is flagged for inspection. In the assembled dataset used for analysis, flagged minutes were rare and, when present, were resolved by inspecting the surrounding values and, if necessary, dropping the ambiguous minute. This procedure keeps the dataset clean without materially affecting the length of any window.

After merging, a final pass of quality checks is performed. These include verifying that the weekend filter has removed all Saturdays and Sundays, confirming that no cross-day carry-forward is present, checking that the first minute of the Polymarket panel meets the liquidity threshold, and ensuring that no duplicate timestamps remain. The result is a set of event-specific, minute-level panels ready for the construction of implied probabilities from the futures series and for time-domain econometric analysis.

\subsection{quality, reproducibility, and limitations}

The principal data quality challenge in this project is the combination of heterogeneous sources and partly manual retrieval. On the Polymarket side, while the price concept is straightforward and the mapping between price and implied probability is direct, a small number of markets exhibit early periods of very low activity that can lead to numerically unstable or uninterpretable price vectors. The twenty-cent threshold applied here is a pragmatic solution that avoids discarding large portions of the window while ensuring that the start of the series reflects meaningful trading interest. Because this trimming is deterministic and documented, it does not compromise reproducibility.

On the futures side, reliance on TradingView and Barchart reflects practical access considerations rather than an ideal of a single, consolidated institutional feed. Both platforms are mature and widely used, but each can occasionally exhibit timestamping quirks, stale updates, or minor discrepancies in extended hours. The dual-source approach mitigates these issues by allowing cross-verification and by providing an alternative when one feed is temporarily incomplete. The explicit conversion from Chicago to New York time for Barchart is necessary and is performed with time-zone-aware libraries to avoid systematic bias from daylight saving time transitions. Nonetheless, a residual limitation is that the precise microsecond ordering of events obviously cannot be guaranteed by retail-grade feeds; the study therefore deliberately avoids drawing inference from sub-minute sequencing.

The decision to remove weekends and to align on a one-minute grid strikes a balance between fidelity and comparability. Removing weekends prevents artificial inference arising from a market that can change when its counterpart cannot trade. The one-minute grid is fine enough to capture meaningful intraday adjustments in beliefs around macroeconomic events without being so fine that the futures series would be riddled with empty bars. Last-observation-carried-forward within a day is a standard device for aligning asynchronous series, but it does introduce a form of step-wise interpolation. Because Granger causality tests are applied to first differences or other stationary transforms in the methodology, and because carry-forward never crosses day boundaries, this interpolation is unlikely to induce spurious predictability. Even so, it is a limitation worth noting: the aligned series reflect a combination of observed trades and within-day holds where no new trade occurs in a minute.

A second limitation is the exclusion of one meeting due to incomplete Polymarket data. The exclusion is justified on quality grounds and preserves the internal validity of the remaining blocks, but it does modestly reduce the sample of events and therefore the external validity of any statements about frequency across meetings. Similarly, the early-period trimming based on the liquidity threshold is necessarily a rule of thumb. Alternative thresholds could be defended; the study’s analysis plan therefore includes robustness checks to confirm that the qualitative conclusions do not hinge on the exact numerical value chosen for that filter.

Reproducibility is addressed in several ways. First, all manual steps are explicitly recorded. These include the list of meeting slugs used for Polymarket retrieval, the decision to exclude the September meeting due to incompleteness, and the time-zone conversions applied to the futures data. Second, programmatic scripts implement the deterministic aspects of the pipeline: reading and reshaping Polymarket outcomes, trimming early observations based on the probability-mass rule, weekend filtering, resampling to minute bars using last-in-minute selection, time-zone normalization, carry-forward within day, and merging by timestamp. Third, where a choice among reasonable alternatives exists, the choice is documented alongside a short justification. Examples include the one-minute grid rather than a five-minute grid at the data stage, the within-day but not cross-day carry-forward, and the use of reconciliation rules when the two futures feeds disagree.

A final class of limitations relates to measurement error and microstructure considerations. Polymarket outcome vectors do not always sum to exactly one because of transaction costs, discrete tick sizes, and the presence of multiple outcome tokens. Renormalization to sum to unity is applied only when the deviation is material and is recorded so that sensitivity analysis can be performed without renormalization. On the futures side, the observed price may reflect transitory microstructure effects such as bid-ask bounce or thin trading during extended hours. Because the econometric analysis focuses on changes and leads and lags at minute resolution, these effects are expected to average out, but they cannot be eliminated entirely in a study that uses publicly accessible feeds.

In summary, the data employed in the study are event-level intraday panels that pair categorical prediction-market prices with standardized federal funds futures prices over windows tailored to each FOMC meeting. The construction process is guided by the principle that both sources should be placed on as equal a footing as possible: the same days, the same time zone, the same sampling grid, and clearly defined, reproducible filters that remove observations unlikely to reflect meaningful beliefs. The resulting datasets are well suited to the subsequent methodology, which converts the futures prices to implied probabilities, aligns the two sources within each event, and tests for directional predictability in a manner that respects both short-run dynamics and longer-run equilibrium relations.
